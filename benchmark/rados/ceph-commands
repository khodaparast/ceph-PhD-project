
#Creating pools erasure codded
$ ceph osd pool create <pg-num> <pgp-num> erasure
$ ceph osd pool create ECpool42 64 64 erasure ECprofile42
$ ceph osd pool create ECpool82 64 64 erasure ECprofile82

# Creating pools replicated
$ ceph osd pool create RepPool6 16 16
$ ceph osd pool set RepPool6 size 6
$ ceph osd pool set RepPool6  min_size 3


# get pool names
$ ceph osd lspools 


# get osd statistics, storage usage, total capacity
ceph osd  df


# get pools statistics, MAX storage, etc.
ceph df


# delete objects in a pool
rados -p ECpool42 cleanup
_____________________________________________________________________
#remove a pool
$ ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
$ example: ceph osd pool delete rbdbench rbdbench --yes-i-really-really-mean-it

_____________________________________________________________________
# state of pages
ceph pg stat

# getting pools crush rules
ceph osd crush dump

_______________________________ purge ceph ___________________________

# following command remove ceph,
ansible-playbook purge-cluster.yml -e ireallymeanit=yes


# if accidentally docker image is removed, then use following command (never do that!!!!!!!!!!! it cause trouble)
ansible-playbook purge-cluster.yml -e ireallymeanit=yes --skip-tags=remove_img


________________________________ Benchmarking with rbd ____________________________

# the following command builds a block device with 1GB size
 
$ rbd create block-device1 --size 10240 --pool rbd2bench --image-feature layering

$ rbd info --image block-device1 --pool rbd2bench

$ rbd map block-device1

$ rbd showmapped
 

$ ceph auth get-or-create client.ecpool42 mon 'allow r, allow command "osd blacklist"' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.ecpool42.keyring


rbd bench –io-type <read | write | readwrite | rw> [–io-size size-in-B/K/M/G/T] [–io-threads num-ios-in-flight] [–io-total size-in-B/K/M/G/T] [–io-pattern seq | rand] [–rw-mix-read read proportion in readwrite] image-spec


$ rbd bench --io-type read --io-size 4M --io-pattern seq  image01 --pool=rbd2bench > rbd-read-4M-seq.txt

$ rbd bench --io-type read --io-size 4M --io-pattern rand  image01 --pool=rbd2bench > rbd-read-4M-rand.txt


$ rbd bench --io-type read --io-size 4M  --io-threads 1 --io-pattern rand  image01 --pool=rbd2bench > rbd-read-4M-rand-singleThread.txt


$ rbd bench --io-type write --io-size 4M --io-pattern rand  image01 --pool=rbd2bench > rbd-write-4M-rand.txt

$ rbd bench --io-type write --io-size 4M --io-threads 1 --io-pattern seq  image01 --pool=rbd2bench > rbd-write-4M-seq-singleThread.txt


$rbd bench --io-type rw --io-size 4M --io-threads 1 --io-pattern seq  image01 --pool=rbd2bench > rbd-rw-4M-seq-singleThread.txt


________________________________ Erasure Coding _____________________________________

Reference: http://www.programmersought.com/article/4629806275/


1. Create an erasure code configuration file:
$ ceph  osd erasure-code-profile  set Ecprofile crush-failure-domain=osd k=3 m=2



2. view the configuration file
$ ceph osd erasure-code-profile ls

Ecprofile
default



$ ceph osd erasure-code-profile get  Ecprofile

crush-device-class=
crush-failure-domain=osd
crush-root=default
jerasure-per-chunk-alignment=false
k=3
m=2
plugin=jerasure
technique=reed_sol_van
w=8




3. Create a new erasure type Ceph pool based on the erasure code configuration file generated in the previous step:

$ ceph osd pool create Ecpool 16 16 erasure Ecprofile

pool 'Ecpool' created





4. Check the status of the newly created pool. You will find that the size of the pool is 5 (k+m), that is, the size of the erasure is 5. Therefore, the data will be written to five different OSDs:

$ ceph osd dump | grep Ecpool 

pool 8 'Ecpool' erasure size 5 min_size 4 crush_rule 3 object_hash rjenkins pg_num 16 pgp_num 16 last_change 231 flags hashpspool stripe_width 12288



5. Now we create a file and put it in the erasure code pool.
$ echo test > test
$ ceph osd pool ls

Ecpool

$ rados put -p Ecpool object1 test
$ rados  -p  Ecpool ls

object1




# delete a profile 
$ ceph osd erasure-code-profile rm ECprofile42



# get the default EC feature
$ ceph osd erasure-code-profile get default 
$ ceph osd erasure-code-profile  ls 


# define EC profile
$ ceph osd erasure-code-profile set ECprofile82  k=8   m=2

# delete content of a pool
rados purge Ecpool21 --yes-i-really-really-mean-it



________________________________ Benchmarking rados ____________________________

# https://blog.codecentric.de/en/2014/03/ceph-object-storage-fast-gets-benchmarking-ceph/

# rados write benchmarking
$ rados bench -p Ecpool82 10 write --no-cleanup seq > Ecpool82_w10.txt 


# rados read benchmarking
$ rados bench -p Ecpool42 10 seq


$ rados -p ECpool42 cleanup
 
$ rados df

$ rados df

# it displays the content of a pool
$ rados -p foo ls -

# it stores the file "osd-test.txt" into clusters with the name myObj1
rados -p ECpool42 put myObj1 osd-test.txt
___________________________________ Caution ___________________________________

# What I removed accideantally and the problem happend!!!!!

$ rbd rm block-device1 --pool rbd2bench
$ rbd rm image01 --pool rbd2bench

$ rbd ls --pool rbd2bench
$ ceph auth rm client.admin

_______________________________________________________________________________


https://docs.ceph.com/docs/jewel/rados/operations/erasure-code/

It is not possible to create an RBD image on an erasure coded pool because it requires partial writes. It is however possible to create an RBD image on an erasure coded pools when a replicated pool tier set a cache tier:


________________________________________ errors___________________________
# error:
application not enabled on 1 pool(s) ceph

# solution: 
ceph osd pool application enable block-devices rbd
_____________________________________________________________________
# error:
If an OSD is down, start it:
# solution: 
sudo systemctl start ceph-osd@1
_____________________________________________________________________
# error:
rbd: sysfs write failed
RBD image feature set mismatch. 1
# solution: 
rbd feature disable rbd2bench/image01 object-map fast-diff deep-flatten


































